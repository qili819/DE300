{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "350c3f8e-3fe9-43bb-89c8-7e3a8010904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "import pyspark.sql.functions as F\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9967702d-ada8-4150-89e3-d89d7520817b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/demos/bin/python3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Python path\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98905ad-b676-492f-b436-5780c9e52261",
   "metadata": {},
   "source": [
    "## Load Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2457c050-c93e-4ba3-934b-78c947fdff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "NUMBER_OF_FOLDS = 5\n",
    "SPLIT_SEED = 7576\n",
    "TRAIN_TEST_SPLIT = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03590a64-a459-4827-9952-3816349c1ba3",
   "metadata": {},
   "source": [
    "## Function for Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5c4c21-c4d5-43dd-a777-f27f79b5bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data; since the data has the header we let spark guess the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV data into a DataFrame\n",
    "    hd_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(DATA_FOLDER,\"*.csv\"))\n",
    "\n",
    "    return hd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0a450-fcb9-4f3a-a354-c983c976238d",
   "metadata": {},
   "source": [
    "## Writing new Transformer type class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2bda3f-a936-459c-9fda-5c7c6579ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner(Transformer):\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        # Select the necessary columns\n",
    "        dt = dataset.select('age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'fbs', \n",
    "                            'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', \n",
    "                            'oldpeak', 'slope', 'num')\n",
    "\n",
    "        # Convert all columns to numeric, handling errors by coercion\n",
    "        for column in dt.columns:\n",
    "            dt = dt.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "        dt = dt.withColumn('num', F.when(F.col('num') == 0, 0).otherwise(1))\n",
    "\n",
    "        # a. Filter painloc and painexer to be either 1 or 0\n",
    "        dt = dt.filter((col('painloc') >= 0) & (col('painexer') >= 0))\n",
    "        dt = dt.withColumn('painloc', when(col('painloc') > 1, 1).otherwise(col('painloc')))\n",
    "        dt = dt.withColumn('painexer', when(col('painexer') > 1, 1).otherwise(col('painexer')))\n",
    "\n",
    "        # b. Replace trestbps < 100 with 0\n",
    "        dt = dt.withColumn('trestbps', when(col('trestbps') < 100, 0).otherwise(col('trestbps')))\n",
    "\n",
    "        # c. Replace oldpeak < 0 with 0 and oldpeak > 4 with 4\n",
    "        dt = dt.withColumn('oldpeak', when(col('oldpeak') < 0, 0).when(col('oldpeak') > 4, 4).otherwise(col('oldpeak')))\n",
    "\n",
    "        # d. Remove rows with missing thaldur and thalach\n",
    "        dt = dt.filter((col('thaldur') != -9) & (col('thalach') != -9))\n",
    "\n",
    "        # e. Remove rows with missing fbs, prop, nitr, pro, diuretic, adjusting prop to be 0 or 1\n",
    "        dt = dt.filter((col('fbs') != -9) & (col('prop') != -9) & (col('nitr') != -9) &\n",
    "                       (col('pro') != -9) & (col('diuretic') != -9))\n",
    "        dt = dt.withColumn('prop', when(col('prop') > 1, 1).otherwise(col('prop')))\n",
    "\n",
    "        # f. Filter exang and slope to be within specific sets\n",
    "        dt = dt.filter(col('exang').isin([0, 1]))\n",
    "        dt = dt.filter(col('slope').isin([1, 2, 3]))\n",
    "\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9138b1-74d5-499e-879e-2e0aed3722f5",
   "metadata": {},
   "source": [
    "## The ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa656f31-fa77-4dca-9473-fb7334fbac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data: DataFrame):\n",
    "    # Step 1: Data Cleaning\n",
    "    cleaner = DataCleaner()\n",
    "    cleaned_data = cleaner.transform(data)\n",
    "\n",
    "    # Step 2: Feature Engineering\n",
    "    feature_columns = [col for col in cleaned_data.columns if col != 'num']\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "    # Step 3: Define Classifiers\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='num')\n",
    "    rf = RandomForestClassifier(featuresCol='features', labelCol='num')\n",
    "\n",
    "    # Step 4: Pipeline Setup\n",
    "    pipeline_lr = Pipeline(stages=[assembler, lr])\n",
    "    pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "    # Step 5: Hyperparameter Tuning Setup\n",
    "    paramGrid_lr = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "\n",
    "    paramGrid_rf = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 20, 50]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "    # Step 6: Cross-Validation Setup\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='num', rawPredictionCol=\"rawPrediction\", metricName='areaUnderROC')\n",
    "    cv_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=paramGrid_lr, evaluator=evaluator, numFolds=NUMBER_OF_FOLDS, seed=SPLIT_SEED)\n",
    "    cv_rf = CrossValidator(estimator=pipeline_rf, estimatorParamMaps=paramGrid_rf, evaluator=evaluator, numFolds=NUMBER_OF_FOLDS, seed=SPLIT_SEED)\n",
    "\n",
    "    # Step 7: Train-Test Split\n",
    "    train_data, test_data = cleaned_data.randomSplit([TRAIN_TEST_SPLIT, 1-TRAIN_TEST_SPLIT], seed=SPLIT_SEED)\n",
    "\n",
    "    # Step 8: Fit the models on the training data\n",
    "    model_lr = cv_lr.fit(train_data)\n",
    "    model_rf = cv_rf.fit(train_data)\n",
    "\n",
    "    # Step 9: Model Evaluation on test data\n",
    "    predictions_lr = model_lr.transform(test_data)\n",
    "    predictions_rf = model_rf.transform(test_data)\n",
    "\n",
    "    auc_lr = evaluator.evaluate(predictions_lr)\n",
    "    auc_rf = evaluator.evaluate(predictions_rf)\n",
    "\n",
    "    # Step 10: Select the better model based on AUC and print the best parameters\n",
    "    if auc_lr > auc_rf:\n",
    "        best_model = model_lr\n",
    "        best_auc = auc_lr\n",
    "        best_model_name = \"Logistic Regression\"\n",
    "        best_params = best_model.bestModel.stages[-1].extractParamMap()\n",
    "    else:\n",
    "        best_model = model_rf\n",
    "        best_auc = auc_rf\n",
    "        best_model_name = \"Random Forest\"\n",
    "        best_params = best_model.bestModel.stages[-1].extractParamMap()\n",
    "\n",
    "    # Print the details of the best model\n",
    "    print(f\"The best model is: {best_model_name}\")\n",
    "    print(f\"AUC of the best model: {best_auc}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de777c-5a00-4c74-9eea-6d5f3d9fa981",
   "metadata": {},
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14ceebde-5c3f-4718-9649-dfb7ef993683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is: Logistic Regression\n",
      "AUC of the best model: 0.8\n",
      "Best parameters:\n",
      "LogisticRegression_10645af2273b__aggregationDepth: 2\n",
      "LogisticRegression_10645af2273b__elasticNetParam: 0.0\n",
      "LogisticRegression_10645af2273b__family: auto\n",
      "LogisticRegression_10645af2273b__featuresCol: features\n",
      "LogisticRegression_10645af2273b__fitIntercept: True\n",
      "LogisticRegression_10645af2273b__labelCol: num\n",
      "LogisticRegression_10645af2273b__maxBlockSizeInMB: 0.0\n",
      "LogisticRegression_10645af2273b__maxIter: 100\n",
      "LogisticRegression_10645af2273b__predictionCol: prediction\n",
      "LogisticRegression_10645af2273b__probabilityCol: probability\n",
      "LogisticRegression_10645af2273b__rawPredictionCol: rawPrediction\n",
      "LogisticRegression_10645af2273b__regParam: 0.1\n",
      "LogisticRegression_10645af2273b__standardization: True\n",
      "LogisticRegression_10645af2273b__threshold: 0.5\n",
      "LogisticRegression_10645af2273b__tol: 1e-06\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Predict Heart Disease\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    data = read_data(spark)\n",
    "    pipeline(data)\n",
    "\n",
    "    spark.stop()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844d943-48cf-4ddc-a06e-8392a8a19389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
